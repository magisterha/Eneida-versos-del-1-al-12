import streamlit as st
import google.generativeai as genai
import requests

# 1. CONFIGURACIÃ“N DE LA PÃGINA
st.set_page_config(
    page_title="Aeneis Tutor AI",
    page_icon="ğŸ›ï¸",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# 2. ESTILO CSS PERSONALIZADO (Look & Feel de FilologÃ­a ClÃ¡sica)
st.markdown("""
    <style>
    .latin-container {
        background-color: #fdfaf3;
        padding: 30px;
        border-radius: 15px;
        border-left: 8px solid #8e44ad;
        box-shadow: 2px 2px 10px rgba(0,0,0,0.05);
        font-family: 'serif';
    }
    .verse-line {
        font-size: 1.25rem;
        line-height: 1.8;
        color: #2c3e50;
        margin-bottom: 5px;
    }
    .main-header {
        color: #8e44ad;
        text-align: center;
        font-weight: bold;
    }
    </style>
    """, unsafe_allow_html=True)

# 3. CARGA DEL PROMPT MAESTRO DESDE GITHUB
@st.cache_data
def load_master_prompt(url):
    try:
        # Reemplaza esta URL con tu enlace "Raw" real de GitHub
        response = requests.get(url)
        if response.status_code == 200:
            return response.text
        else:
            return "Error: No se pudo cargar el Prompt Maestro."
    except Exception as e:
        return f"Error de conexiÃ³n: {str(e)}"

# --- CONFIGURACIÃ“N DE URL Y API ---
# SUSTITUYE POR TU URL REAL
GITHUB_RAW_URL = "https://raw.githubusercontent.com/tu_usuario/tu_repo/main/prompt_maestro.txt"
master_prompt_content = load_master_prompt(GITHUB_RAW_URL)

# ConfiguraciÃ³n de Gemini
if "GEMINI_API_KEY" in st.secrets:
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
    # Usando el modelo Flash Lite disponible a finales de 2025
    model = genai.GenerativeModel(
        model_name="gemini-2.0-flash-lite-preview-02-05",
        system_instruction=master_prompt_content
    )
else:
    st.error("âš ï¸ Configura tu GEMINI_API_KEY en los Secrets de Streamlit.")
    st.stop()

# 4. INTERFAZ DE USUARIO (DASHBOARD)
st.markdown("<h1 class='main-header'>ğŸ›ï¸ Proyecto Eneida: TutorÃ­a SocrÃ¡tica</h1>", unsafe_allow_html=True)
st.divider()

col_texto, col_chat = st.columns([1, 1], gap="large")

# --- COLUMNA IZQUIERDA: EL TEXTO ---
with col_texto:
    st.subheader("P. Vergili Maronis: Aeneis (I, 1-11)")
    
    texto_latino = [
        "1. Arma virumque canÅ, TrÅiae quÄ« prÄ«mus ab ÅrÄ«s",
        "2. Äªtaliam, fÄtÅ profugus, LÄvÄ«niaque vÄ“nit",
        "3. lÄ«tora, multum ille et terrÄ«s iactÄtus et altÅ",
        "4. vÄ« superum saevae memorem IÅ«nÅnis ob Ä«ram;",
        "5. multa quoque et bellÅ passus, dum conderet urbem,",
        "6. inferretque deÅs LatiÅ, genus unde LatÄ«num,",
        "7. AlbÄnÄ«que patrÄ“s, atque altae moenia RÅmae.",
        "8. MÅ«sa, mihÄ« causÄs memorÄ, quÅ nÅ«mine laesÅ,",
        "9. quidve dolÄ“ns, rÄ“gÄ«na deum tot volvere cÄsÅ«s",
        "10. Ä«nsÄ«gnem pietÄte virum, tot adÄ«re labÅrÄ“s",
        "11. impulerit. Tantaene animÄ«s caelestibus Ä«rae?"
    ]
    
    with st.container():
        st.markdown('<div class="latin-container">', unsafe_allow_html=True)
        for line in texto_latino:
            st.markdown(f'<div class="verse-line">{line}</div>', unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)
    
    st.info("ğŸ’¡ **Consejo:** Empieza saludando en el chat para definir el idioma de la tutorÃ­a.")

# --- COLUMNA DERECHA: EL CHAT ---
with col_chat:
    st.subheader("ğŸ’¬ GuÃ­a FilolÃ³gica Interactiva")
    
    # InicializaciÃ³n del historial
    if "messages" not in st.session_state:
        st.session_state.messages = []
        # EjecuciÃ³n del protocolo inicial (saludo en latÃ­n y pregunta de idioma)
        try:
            initial_response = model.generate_content("EJECUTA PROTOCOLO INICIAL.")
            st.session_state.messages.append({"role": "assistant", "content": initial_response.text})
        except Exception as e:
            st.error(f"Error al conectar con Gemini: {e}")

    # Mostrar mensajes del historial
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Input de chat
    if prompt := st.chat_input("Escribe tu duda o respuesta..."):
        # Guardar mensaje del usuario
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Generar respuesta de la IA
        with st.chat_message("assistant"):
            # Pasamos el historial completo para mantener la lÃ³gica de bloques y transiciones
            history = [
                {"role": m["role"], "parts": [m["content"]]} 
                for m in st.session_state.messages[:-1]
            ]
            chat = model.start_chat(history=history)
            
            with st.spinner("Analizando sintaxis..."):
                response = chat.send_message(prompt)
                st.markdown(response.text)
                st.session_state.messages.append({"role": "assistant", "content": response.text})

    # BOTÃ“N DE RESERVA (Footer del chat)
    st.divider()
    form_link = "https://docs.google.com/forms/d/e/1FAIpQLSdcEGs0k3eO1A3yDwwlRPZxM7RPpOPVD121J6GMUwAgbtbQ5w/viewform?usp=header"
    st.link_button("ğŸ« Reserva una clase con un profesor de latÃ­n", form_link, use_container_width=True, type="primary")
