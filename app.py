import streamlit as st
import google.generativeai as genai
import requests

# 1. CONFIGURACIÃ“N DE PÃGINA
st.set_page_config(page_title="Aeneis Tutor AI", layout="wide")

# 2. CSS PARA COLUMNA FIJA Y ESTÃ‰TICA
st.markdown("""
    <style>
    [data-testid="column"]:nth-of-type(1) {
        position: sticky;
        top: 2rem;
        align-self: flex-start;
    }
    .verse-line { font-family: 'serif'; font-size: 1.4rem; line-height: 1.7; color: #2c3e50; }
    .main-header { color: #8e44ad; font-weight: bold; }
    </style>
    """, unsafe_allow_html=True)

# 3. SELECCIÃ“N DE IDIOMA EN EL FRONTEND
with st.sidebar:
    st.title("âš™ï¸ ConfiguraciÃ³n")
    idioma_app = st.radio(
        "Selecciona el idioma del Tutor:",
        ["EspaÃ±ol", "English", "Latine", "ç¹é«”ä¸­æ–‡ (Taiwan)"],
        index=0
    )
    st.info(f"El tutor responderÃ¡ preferentemente en {idioma_app}.")
    if st.button("ğŸ”„ Reiniciar Chat"):
        st.session_state.messages = []
        st.rerun()

# 4. CONFIGURACIÃ“N DE AI
@st.cache_data
def load_prompt(url):
    try:
        r = requests.get(url)
        return r.text if r.status_code == 200 else "Eres un tutor de latÃ­n experto."
    except: return "Eres un tutor de latÃ­n experto."

# URL RAW de tu GitHub para el prompt maestro
PROMPT_URL = "https://raw.githubusercontent.com/tu_usuario/tu_repo/main/prompt_maestro.txt"
sys_instruction = load_prompt(PROMPT_URL)

if "GEMINI_API_KEY" in st.secrets:
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
    model = genai.GenerativeModel(
        model_name="gemini-2.0-flash-lite-preview-02-05",
        system_instruction=sys_instruction
    )
else:
    st.error("âš ï¸ Configura la API KEY en Secrets.")
    st.stop()

# 5. DISEÃ‘O DE PANTALLA DIVIDIDA
col_txt, col_chat = st.columns([1, 1], gap="large")

with col_txt:
    st.markdown("<h2 class='main-header'>P. Vergili Maronis: Aeneis (I, 1-11)</h2>", unsafe_allow_html=True)
    st.write("---")
    versos = [
        "1. Arma virumque canÅ, TrÅiae quÄ« prÄ«mus ab ÅrÄ«s",
        "2. Äªtaliam, fÄtÅ profugus, LÄvÄ«niaque vÄ“nit",
        "3. lÄ«tora, multum ille et terrÄ«s iactÄtus et altÅ",
        "4. vÄ« superum saevae memorem IÅ«nÅnis ob Ä«ram;",
        "5. multa quoque et bellÅ passus, dum conderet urbem,",
        "6. inferretque deÅs LatiÅ, genus unde LatÄ«num,",
        "7. AlbÄnÄ«que patrÄ“s, atque altae moenia RÅmae.",
        "8. MÅ«sa, mihÄ« causÄs memorÄ, quÅ nÅ«mine laesÅ,",
        "9. quidve dolÄ“ns, rÄ“gÄ«na deum tot volvere cÄsÅ«s",
        "10. Ä«nsÄ«gnem pietÄte virum, tot adÄ«re labÅrÄ“s",
        "11. impulerit. Tantaene animÄ«s caelestibus Ä«rae?"
    ]
    for v in versos: st.markdown(f'<p class="verse-line">{v}</p>', unsafe_allow_html=True)

with col_chat:
    st.subheader("ğŸ’¬ Consulta FilolÃ³gica Libre")
    
    chat_container = st.container(height=600, border=True)

    if "messages" not in st.session_state or len(st.session_state.messages) == 0:
        welcome = f"### ğŸ›ï¸ Salve!\nSoy tu tutor experto. He configurado mi respuesta en **{idioma_app}**.\n\nÂ¿QuÃ© te gustarÃ­a analizar de estos versos?"
        st.session_state.messages = [{"role": "assistant", "content": welcome}]

    with chat_container:
        for m in st.session_state.messages:
            with st.chat_message(m["role"]): st.markdown(m["content"])

    # LLAMADO A LA ACCIÃ“N (CTA) PERSONALIZADO
    if prompt := st.chat_input("PregÃºntale cualquier cosa de este texto a la IA"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with chat_container:
            with st.chat_message("user"): st.markdown(prompt)

            with st.chat_message("assistant"):
                # Enviamos el contexto del idioma seleccionado en el frontend
                context_msg = f"[Idioma de respuesta: {idioma_app}] {prompt}"
                
                history = [{"role": m["role"], "parts": [m["content"]]} for m in st.session_state.messages[:-1]]
                chat = model.start_chat(history=history)
                
                try:
                    with st.spinner("Analizando..."):
                        response = chat.send_message(context_msg)
                        st.markdown(response.text)
                        st.session_state.messages.append({"role": "assistant", "content": response.text})
                except Exception as e:
                    st.error("ğŸ›ï¸ El orÃ¡culo estÃ¡ saturado. Intenta de nuevo en un minuto.")
        st.rerun()
