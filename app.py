import streamlit as st
import google.generativeai as genai
import requests

# 1. ConfiguraciÃ³n de la pÃ¡gina (Layout ancho para dividir la pantalla)
st.set_page_config(
    page_title="Aeneis Tutor AI",
    page_icon="ğŸ›ï¸",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# 2. CSS PERSONALIZADO: EstÃ©tica y fijaciÃ³n de texto
st.markdown("""
    <style>
    /* Estilo para los versos de la Eneida */
    .verse-line {
        font-family: 'Times New Roman', serif;
        font-size: 1.4rem;
        line-height: 1.8;
        color: #2c3e50;
        margin-bottom: 5px;
    }
    .main-header {
        color: #8e44ad;
        font-weight: bold;
        margin-bottom: 20px;
    }
    /* Ocultar elementos innecesarios para mÃ¡xima limpieza */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    </style>
    """, unsafe_allow_html=True)

# 3. Carga del Prompt Maestro desde GitHub
@st.cache_data
def load_prompt(url):
    try:
        r = requests.get(url)
        if r.status_code == 200:
            return r.text
        return "Error: No se pudo cargar el Prompt Maestro."
    except Exception as e:
        return f"Error de conexiÃ³n: {str(e)}"

# REEMPLAZA CON TU URL REAL DE GITHUB
GITHUB_RAW_URL = "https://raw.githubusercontent.com/tu_usuario/tu_repo/main/prompt_maestro.txt"
master_prompt_content = load_prompt(GITHUB_RAW_URL)

# InicializaciÃ³n de Gemini 2.5 Flash Lite (ConfiguraciÃ³n para diciembre 2025)
if "GEMINI_API_KEY" in st.secrets:
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
    model = genai.GenerativeModel(
        model_name="gemini-2.0-flash-lite-preview-02-05", 
        system_instruction=master_prompt_content
    )
else:
    st.error("âš ï¸ Configura la API KEY en los Secrets de Streamlit.")
    st.stop()

# 4. DISEÃ‘O DE PANTALLA DIVIDIDA
col_texto, col_chat = st.columns([1, 1], gap="large")

# --- COLUMNA IZQUIERDA: TEXTO FIJO ---
with col_texto:
    st.markdown("<h2 class='main-header'>P. Vergili Maronis: Aeneis (I, 1-11)</h2>", unsafe_allow_html=True)
    
    versos = [
        "1. Arma virumque canÅ, TrÅiae quÄ« prÄ«mus ab ÅrÄ«s",
        "2. Äªtaliam, fÄtÅ profugus, LÄvÄ«niaque vÄ“nit",
        "3. lÄ«tora, multum ille et terrÄ«s iactÄtus et altÅ",
        "4. vÄ« superum saevae memorem IÅ«nÅnis ob Ä«ram;",
        "5. multa quoque et bellÅ passus, dum conderet urbem,",
        "6. inferretque deÅs LatiÅ, genus unde LatÄ«num,",
        "7. AlbÄnÄ«que patrÄ“s, atque altae moenia RÅmae.",
        "8. MÅ«sa, mihÄ« causÄs memorÄ, quÅ nÅ«mine laesÅ,",
        "9. quidve dolÄ“ns, rÄ“gÄ«na deum tot volvere cÄsÅ«s",
        "10. Ä«nsÄ«gnem pietÄte virum, tot adÄ«re labÅrÄ“s",
        "11. impulerit. Tantaene animÄ«s caelestibus Ä«rae?"
    ]
    
    # Renderizado estÃ¡tico
    for v in versos:
        st.markdown(f'<div class="verse-line">{v}</div>', unsafe_allow_html=True)
    
    st.divider()
    st.caption("ğŸ“ El texto de la Eneida permanece estÃ¡tico para facilitar su consulta.")

# --- COLUMNA DERECHA: CHAT CON SCROLL INDEPENDIENTE ---
with col_chat:
    st.subheader("ğŸ’¬ GuÃ­a FilolÃ³gica Interactiva")
    
    # Contenedor de chat con altura fija para permitir scroll independiente
    chat_container = st.container(height=600, border=True)

    if "messages" not in st.session_state:
        welcome_text = "### ğŸ›ï¸ Salve, discipule!\nPor favor, elige tu idioma para comenzar:\n* **EspaÃ±ol** | **English** | **Latine** | **ç¹é«”ä¸­æ–‡**"
        st.session_state.messages = [{"role": "assistant", "content": welcome_text}]

    # Mostrar mensajes dentro del contenedor con scroll
    with chat_container:
        for m in st.session_state.messages:
            with st.chat_message(m["role"]):
                st.markdown(m["content"])

    # Entrada de chat (fuera del contenedor de mensajes para que siempre estÃ© visible abajo)
    if prompt := st.chat_input("Escribe tu idioma o respuesta..."):
        # Guardar y mostrar mensaje del usuario
        st.session_state.messages.append({"role": "user", "content": prompt})
        with chat_container:
            with st.chat_message("user"):
                st.markdown(prompt)

            # Generar respuesta de la IA
            with st.chat_message("assistant"):
                if len(st.session_state.messages) <= 2:
                    command = f"El idioma elegido es {prompt}. ANALIZA AHORA el Bloque 1."
                else:
                    command = prompt

                history = [{"role": m["role"], "parts": [m["content"]]} for m in st.session_state.messages[:-1]]
                chat = model.start_chat(history=history)
                
                with st.spinner("Consultando al tutor..."):
                    response = chat.send_message(command)
                    st.markdown(response.text)
                    st.session_state.messages.append({"role": "assistant", "content": response.text})
        
        # Forzar recarga para que el contenedor se deslice al final
        st.rerun()

    # BotÃ³n de reserva (Anclado al final de la columna de chat)
    st.divider()
    cta_url = "https://docs.google.com/forms/d/e/1FAIpQLSdcEGs0k3eO1A3yDwwlRPZxM7RPpOPVD121J6GMUwAgbtbQ5w/viewform?usp=header"
    st.link_button("ğŸ›ï¸ Reserva una clase con un profesor de latÃ­n", cta_url, use_container_width=True, type="primary")
